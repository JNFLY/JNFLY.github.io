
# Physics-Informed Machine Learning: A Survey on Problems, Methods and Applications $\star \star \star$

2022 by Z. Hao, S. Liu, Y. Zhang, C. Ying, Y. Feng, H. Su, et al.
## 1. Introduction

First: The definition / representation of physical prior knowledge: 
1. PINN's **loss function** uses PDE/ODEs as regularization terms.
2. NeuralODE uses a novel **architechture** that obeys ODEs.
3. **Symmetry constraints** and **intuitive physical constraints**, like permutation invariance, conservation laws, and physical common sense...
   
Second: How does the prior knowledge go into ML model?
1. **Data augmentation** by symmetry constraints or known PDE/ODE
2. **Redesign model architechture** that suits the current constraints.
3. **Loss function and optimization mathod** need to be changed, like **weight adjustment** become very important, **Adam** not suitable anymore[<sup>Adam</sup>](#AdamNotSuitable)

Third: What is the task of PIML?
1. **Condition:** must involve real-world physical processes (It needs to have physics)
2. **Solving scientific problems:** neural simulations and inverse problems
3. Some other paper: PIML by G. E. Karniadakis[<sup>PIML</sup>](#PIML)... (Find more to read here)

## 2. Neural Simulations

**Traditional** method: FDM for ODE(with RK), FVM and FEM (usually used on mesh-based models), **Spectral method**(a meshless method). A crude estimate of the complexity of them is given by $\mathcal{O}(dn^r)$ where $r \approx3$ 

**Neural Network-based** method: 1. (maybe) resist the curse of dimensionality. 2. high-dimensional data can (potentially) be well approximated by a much lower-dimensional manifold.

### 2.1 Neural Solver
**Def**: use NN to solve problem with physics **completely known**.

Still in development, lack of **computational efficiency, accuracy, and convergence guarantee** compared to FEM, FVM.

The setting of first PINN in[<sup>FirstPINN</sup>](#FirstPINN): 
1. **Loss:** residual, initial/boundary condition, and data fitting loss
2. **Learning rate** as $\lambda_r$, $\lambda_i$, $\lambda_b$, $\lambda_d$, not adaptive.
3. **Automatic differentiation** to calculate derivative terms
4. **Monte_Carlo Sampling** to approximate integrals
5. **SGD, LBFGS** are chosen as optimizers.

Problems & Variants of first PINN:
1. **Different convergence rates of losses**: multi-task learning, reweighting scheme
2. **Different loss design**, add regularization terms...
3. **Specific neural architechture**: LSTM/RNN, mesh-based representation, hard constraint...
4. **Others**: domain decomposition, feature processing technic, transfer learning, meta-learning...

#### 2.1.1 Loss Re-Weighting and Data-Resampling

**Loss re-weighting:**

1. **One famous work**[<sup>Pathology</sup>](#Pathology):  shows that the gradients when training PINNs might be pathological. And they gave an updating rule for the weights.
![Image](lambda_updating.png)

2. **Other attemptations:** Neural Tangeting Kernel, Inverse-Dirichlet Weighting(using the gradient varience), "characteristic quantities"

**Data re-sampling:**
1. **Model-agnostic sampling:** Sample quasi-random points/ low-discrepancy sequence, Sobel sqeuence, Latin hypercube, Halton sequenc, Hammersley sampling, Faure sampling.

2. Sample collocation points from areas with **higher error**: PDE residual loss can be viewed as an (expectation over the) probability distribution. Target: find a **suitable resampling function** $q(x)$
   ![Image](Residual_as_distribution.png)

#### 2.1.2 Novel Optimization Objectives

Idea: **train PINN with novel loss function**
()
1. **Incorporate Numerical Differentiation:** CAN-PINN use upwind/central schemes for convection terms, replacing automatic differentiation; cvPINN borrows the idea in FVM....

2. **Variational formulation:** Use the variational formulation & **Galerkin formulation**(VPINN): The key point is to select proper test functions. In specific, **if choose delta function(center as collocation point)**, VPINN degenerate to PINN.  
(e.g. VarNet: take piecewise linear functions as test; function. <br> hp-VPINN: domain decomposition; **WAN**: solve a min-max problem like **GAN**[<sup>WAN</sup>](#WAN))

3. **Other equivalent form:** BINet: combine boundary integrals equation; 

4. **Regularization terms:** gradient_enhanced training...

#### 2.1.3 Novel Neural Architechtures

1. **Activation function:** $Swish(x) = x \cdot Sigmoid(\beta x)$; $SReLU(x)=x_+(1-x)_+$ (this is with compact support); Other adaptive activation functions (e.g. $\sigma(na\cdot x)$, $a$ is a learnable weight; slope-recovery term)

2. **Feature preprocessing (Embedding):** Fourier features(NTK theory shows that it makes NN easier to learn high-frequency functions)
![Image](Fourier_features.png)

3. **Multiple NN** parameter sharing is suboptimal in the multi-task learning's viewpoint, thus need **multiple MLP**(Multi-layer perceptron)

4. **Boundary Encoding:**  Postprocessing layers for encoding b.c./i.c. with **hard constraint**(Encoding PDEs with hard constraint is hard). e.g. add layer $u_w' = u_w \cdot x(L-x)$ at last to encode Dirichlet b.c.; Represent the outputs on the basis of Fourier. **Also**,  [<sup>Hard_constraint</sup>](#Hard_constraint)gives a unified framework for encoding regular b.c. in complex domain (Decompose the solution into $u(\bold{x}) = v(\bold{x}) + D(\bold{x})y(\bold{x})$, and train $v$ with b.c. loss, $y$ with residual loss seperatly, also carefully choose a $D(x)$ that vanishes on the boundary)

5. **Sequential Neural Architechture:** Solving **time-dependent PDEs**: RNN, LSTM, Variational auto-encoder and Hamiltonian neural network(HNN)

6. **Convolutional Architechtures**: One can discretize state variable $u$ and then naturally use CNN. **Limitation:** Hard to be implemented on irregular domain. (Find bijection to the rectangular domain; Graph network)

7. **Domain Decomposition:** Effective on a large-scale problems/ multi-scale problems. **cPINN** assumes the conservation law holds and designs a interface loss. **XPINN** does't require that condition. In the loss design, it enforeces the continuity of **general PDE residuals** rather than flux continuity

#### 2.1.4 Challenges and Future Work

    Many works are far from being fully explored:

1. **Optimization Process:** Training process is significantly different from the ordinary NN. Existing discussion is not enough to build a stable and effective solver.

2. **Model Architechture:** Many novel architechture in DL are not implemented, like normalization layer and transformer architechture.

3. **Solving High-dimensional Problems:** Like HJB equations and Schrodinger equations are notoriously hard to solve in high-dimension.


### 2.2 Neural Operator

Goal: To learn a surrogate model representing ODEs/PDEs for all $\theta \in \Theta$ rather than just solving a single physical system. 

#### 2.2.1 Direct Methods

1. **DeepONets:** $G_w(\theta)(\boldsymbol{x}) = b_0 + \sum_{k=1}^p b_k(\theta)t_k(\boldsymbol{x})$, $(b_k)$ is the branck network, $(t_k)$ is the trunk network. Train with supervised data.**Pros:** only a single foward pass is needed, mesh-less; **Cons:** large training data.

2. **Physics_informed DeepONets:** Train with a combination of data and physics-informed losses.

3. **Improved architectures for DeepONets:** Encoder/Auto-Decoder; Pre-processing; Post-processing(e.g. used in constructing hard-constraint);

4. **Multiple-input DeepONets(MIONet):** To extends DeepONets so that $\theta$ can be in different (Banach) spaces (e.g. multiple-vector, vector-valued function). A new universial approximation thm has been proved.

5. **Pre-trained DeepONets for multi-physics:** DeepM&Mnets, using several pre-trained DeepONet as building blocks for multi-physical systems. Very suitable for performing "physically meaningful" interpolation.

#### 2.2.2 Green's Function Learning

1. **Green's function:** Can be considered as a subclass of operator learning: with forcing term $f$ being the parameter.

2. **For linear operator:** **Pros:** The structure contains more prior information/ Mathematically easier to solve/Some property can be added flexibly(Symmetry). **Cons:** Input dimension is twice the spatial dimension.

3. **For nonlinear operators:** discretize the boundary problem and use e.g. autoencoder to linearize the target operator.

#### 2.2.3 Grid-based Opearator Learning
We can formalize the latent operator as a grid-based mapping, called image-to-image mapping:
$$\tilde{G}: \theta = \{v(x_i) \}_{i=1}^N \mapsto \{u(x_i)\}_{i=1}^N$$

1. **Convolutional network:** U-Net... Can extract segmentation
behind the training “images” and learn a low-rank representation of the physical laws. **Cons:** suffers from "The curse of dimensionality", hard to utilize frequency information, hard for geometrically complex problems. 

2. **Fourier neural operator:** Another architechture to learn image-to-image mapping (Then becomes the paradigm now)
   ![Image](FNO.png)

3. **With attention mechanism:** query, key, value. 

**Big model for PINN is hard:** Sampling from a physical system is usually expensive. we usually don't have that much sample.

#### 2.2.4 Graph-based Operator Learning
Discretize the input/output function in some graph 
$\mathcal{G} = \{ \mathcal{V}, \mathcal{E} \}$, $\mathcal{G} is learnable?$
* Graph Kernel Network, Multipole Graph Neural Operator, Graph Neural Opeartor with Autogressive Methods

#### 2.2.5 Open Challenges and Future Work

1. **Incorporating physical priors:** Speed and accuracy are still far inferior to traditional numerical method. Need theoretically improvement to the framework.

2. **Reducing the cost of gathering dataset**
3. **Developing large pre-trained model:** Want some once-for-all model.
4. **Modeling real-world physical systems:** Bring more challenges
including geometrically complex problems, high-dimensional problems, chaotic problems, long-term prediction.

### 2.3 Theory

#### 2.3.1 Expression Ability
For machine learning, the first thing that we care is to find **a proper hypothesis set** to the problem.
1. **Universal approximation theorem for DeepONet:**  one-layer neural networks can approximate any operator to arbitrary accuracy
   
2. **Shallow neural network may have bad performance** 

3. **Only need width to be $\mathcal{O}(M+N+L)$**

#### 2.3.2 Convergence

1. Using the regularized empirical loss for bounding the expected PINN loss.
2. Some other general convergence analysis

#### 2.3.3 Error Estimation
**A general form for DeepONet:** Encoder, Approximator, Reconstructor
The exact error L2 error $\hat{\mathcal{E}}$ can **be bounded by analyzing the above three errors**.

#### 2.3.4 Future Work
1. **Expression ability:** Why deep net has better expression ability
2. **Convergence:** Currently very preliminary since the stability of PDEs itself is complicated. Need to explore more on the convergence analysis of PINN/DeepONet
3. **Error estimation:** , approximation error and generalization error are attracting more and more attention.

### 2.4 Applications
Fluid Dynamics, Material Science, Medicine, Geography, Industry...

## 3 Inverse Problem

### 3.1 Problem Formulation
![Image](Inverse_design.png)

### 3.2 Neural Surrogate Models
Construct a **surrogate model** to optimize inverse design instead of using the numerical solver. (NN here used to design a system or embed physical knowledge)

1. **With PINN:** hPINN take the hard constraint and use the augmented Lagrangian method. Also, many paper will regard $\theta$ as the control parameter. Also combined with optimization strategies.

2. **With Neural Operators** Avoid repeatedly calling numerical solver for PDE constraints. AmorFEA: based on Galerkin minimization formulation of PDEs, NN trained to minimize the expected potential energy. Two stages optimization framework.

3. **With Neural Simulator:** learn a mapping $S: \mathcal{\theta} \to \mathbb{R}^n$, which is from the input parameters/functions to the evaluation (even directly to the objective function). ISMO

### 3.3 Other Models
NN also plays different roles in the following ways:
1. **Neural Rpresentation:** Parameterizing the parameters to be optimized with neural network instead of their original space.

2. **Design Prediction:** Use neural networks to perform inverse design directly

3. **Data Generation:** use deep generative models to obtain a series of satisfying solutions.

### 3.4 Beyond Inverse Design

**Inverse problem:** property identification, parameter inference, science discovery

e.g.  Discover new materials or identify properties of given materials from experimental/simulated data

Learn physical quantities such as velocity and pressure from
flow data

Future work: Neural Surrogate Modeling/ Large Scale Application/ Other directions.

## 4. Computer Vision and Graphics
 1. Taditional Visual Task
 2. Motion and Pose Analysis
 3. Physical Scene Understanding
 4. Computer Graphics

## 5. Reinforcement Learning

The classical formulation of RL is based on Markov Decision Process(MDPs).

### 5.2 Policy Training
Many are from the real-world problem, and we had several understanding which can accelerate the policy training.

### 5.3 Model Training & 5.4 Exploration Guiding
Interesting work like RL-CBF, which uses expert knowlegde to construct a Gaussian Process(GP), modeling the state-dependent uncertainty.

### 5.5 Future Work
1. **Generic Modeling of Physical Tasks:** Build a general model so that we can easily deal with new physical tasks with proper expert knowledge.
2.  **Solving High-Dimensional Problem:** RL with high-dimensional state spaces are not sample-efficient: Resort to compressing the state space. 
3.  **Guaranteeing Safety in Complex, Uncertainty Environment:** A trade-off between complexity and safety gurantee.

## 6. Conclusion

## References

<div id = "AdamNotSuitable"></div>
D. P. Kingma and J. Ba, “Adam: A method for stochastic opti- mization,” arXiv preprint arXiv:1412.6980, 2014. <br>

<div id = "PIML"></div>
G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang,
and L. Yang, “Physics-informed machine learning,” Nature Reviews Physics, vol. 3, no. 6, pp. 422–440, 2021. <br><br>

<div id = "FirstPINN"></div>
M. Raissi, P. Perdikaris, and G. E. Karniadakis, “Physics-informed
neural networks: A deep learning framework for solving forward
and inverse problems involving nonlinear partial differential
equations,” Journal of Computational Physics, vol. 378, pp. 686–707, <br><br>

<div id = "Pathology"></div>
S. Wang, Y. Teng, and P. Perdikaris, “Understanding and mitigating gradient flow pathologies in physics-informed neural
networks,” SIAM Journal on Scientific Computing, vol. 43, no. 5,
pp. A3055–A3081, 2021. <br><br>

<div id = "WAN"></div>
Y. Zang, G. Bao, X. Ye, and H. Zhou, “Weak adversarial networks
for high-dimensional partial differential equations,” Journal of
Computational Physics, vol. 411, p. 109409, 2020.<br><br>

<div id = "Hard_constraint"></div>
S. Liu, Z. Hao, C. Ying, H. Su, J. Zhu, and Z. Cheng, “A unified
hard-constraint framework for solving geometrically complex
pdes,” arXiv preprint arXiv:2210.03526, 2022.


<br><br><br>

# Physics-Informed Machine Learning  $\star \star \star$

by G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang and L. Yang

## 1. Introduction
Modeling and predicting a **nonlinear multiscale systems** with **inhomogeneous cascades-of-scales** may inevitably face many problems: prohibitive cost, sources of uncertainty.

Why do we need ML: 
1. hard to incorporate **multi-fidelity data** into existing physical models. 
2. **heterogeneity of available data**, along with the lack of **universally acceptable models**
3. link existing **multiphysics and multiscale models** using ML

However, prediction from ML might be physically wrong and lead to poor generalization performance, because of the extrapolation and observational biases. That's why we need **ML models governed by physical rules**

The most interesting category of problem contains **partial physical knowledge and scattered measurement**

### 1.1 Principle of physics-informed learning
**No predictive model can be constructed without assumptions.**

* Observational biases: directly from the data derived from the **underlying physics** or crafted **data augmentation procedures**

* Inductive biases: Tailored intervention to the architechture (e.g. mathematical constraints). **Rmk:** no need to be strictly satisfied since that may limit the model to a simple prior.

* Learning biases: Appropriate choice of loss functions.

![Image](Three_biases.png)

## 1.2 Examples:
**Inductive biases:** Convolutional NN, GNN, Kernel method,
NN architechture related to a viscosity solution of **HJ-PDE**

**Learning biases:** (Can be viewed as multi-task learning) e.g.: Deep Galerkin method, PINNs and its variants, GAN, PI Auto-encoders...

**Hybrid approaches:** DeepONets...

## 1.3 Connections to other methods

**Kernel methods:** NN can be rigorously interpreted as kernel methods where the warping kernel is also learned from data...

**Other classical numerical schemes:** **CNN** are similar to FDM in discretizing the translation-invariant PDEs; **ResNet** are similar to to forward Euler discretization of autonomous ODE. Runge-Kutta schemes are very similar to **recurrent NN**; **DNN** with ReLU activation is equivalent to linear FEM method.

## Merits of Physics-informed learning

* Incomplete models and imperfect data: B-PINN (B for Bayesian (based))
* **Strong generalization** (including extrapolation ability)
* Approaches to understand deep learning: ...
* **Tackling high dimensionality:**










# Understanding and mitigating gradient flow pathologies in  physics-informed neural networks $\star \star$

2021 by S. Wang, Y. Teng, and P. Perdikaris

## 1. A pathological phenomenon
 **Main idea:** The gradients when training PINN are easy to be pathological.

**Training setting:** 
1. Solving Helmholtz equation in 2D.
2. Two losses: residual loss $\mathcal{L} _r$ and boundary loss $\mathcal{L} _b$
3. 4-layer **fully-connected** NN, 50 neural/layers 
4. Optimizer: **Adam**, 40000 steps, learning rate: $10^{-3}$ and a **decrease annealing**

Find NN did **very bad** on approximating the boundary condition. Trying to figure out why.

**Useful Point:** Track the gradients of $\mathcal{L} _r$ and $\mathcal{L} _b$ w.r.t. the weights of each layers:

![Image](track_gradient.png)
$|\nabla_{\theta}\mathcal{L}_b|$ is much smaller which makes it negligible in the training process.

**Conclusion:**  The gradients of each loss should **match each other** in magnitude.

## 2. Go back to 1D Poisson equation

Theoretically, when the **wave frequency** $C$ in the function becomes larger, $||\nabla_{\theta}\mathcal{L}_r||_{L^{\infty}}$ becomes larger, greater than $||\nabla_{\theta}\mathcal{L}_b||_{L^{\infty}}$

Practically it shows the same result. $C$ larger, the result worser.
![Image](Poisson_C.png)

## 3. Stiffness Analysis
**Equivalence** between the gradient descent and the Euler method for **gradient flow**.

Computing the Hessian to know the stiffness/decide the learning rate:
$\eta <2/\sigma_{max}(\nabla_{\theta}^2\mathcal{L}(\theta))$

**Re-weighting algorithm:**...
![Image](lambda_updating.png)
**Novel Architechture:**...

# Solving high-dimensional partial differential equations using deep learning

2022 by Jiequn Hana, Arnulf Jentzenb, and Weinan Ea,c,d,1

**Some important PDEs that are naturally high dimensional:** 

1. **Schrodinger equation:** in many-body problem,dim roughly 3 times of the particle
2. **Black–Scholes equation:** dim is the number of underlying financial assets.
3. **Hamilton–Jacobi–Bellman equation:** dim goes up linearly with agents/resources.

Computational cost goes up exponentially. Essential problems is to approximate a non-linear function in high dimension.
